# -*- coding: utf-8 -*-
"""cs758.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fzGOomuuPlhs51Ap-W70nTtt4r--Z8Fl
"""

# TensorFlow and tf.keras
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib

# Commonly used modules
import numpy as np
import pandas as pd
import os
import sys

from os import listdir
from os.path import isfile, join

# Import dataset (temporary)

out_columns = ['PST', 'TVD', 'Entropy', 'Swaps']

dataset_path = "./dataSets_V1/dataSets_Noise"
files = [join(dataset_path, f) for f in listdir(dataset_path) if isfile(join(dataset_path, f))]

dfs = [pd.read_csv(f) for f in files]
df = pd.concat(dfs)

X = df.drop(columns = out_columns)
Y = df[out_columns]

from sklearn import preprocessing

# Scaling

min_max_scaler = preprocessing.MinMaxScaler()
X_scale = min_max_scaler.fit_transform(X)

from sklearn.model_selection import train_test_split

X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.3)
X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)

import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, BatchNormalization

# Baseline model

Y = Y[['TVD']]
Y_train = Y_train[['TVD']]
Y_test = Y_test[['TVD']]
Y_val = Y_val[['TVD']]

input_size = len(X.columns)
output_size = len(Y.columns)

leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.01)

model = Sequential([
    Dense(512, activation=leaky_relu, input_shape=(input_size,)),
    BatchNormalization(),

    Dense(256, activation=leaky_relu),
    BatchNormalization(),

    Dense(256, activation=leaky_relu),
    BatchNormalization(),

    Dense(128, activation=leaky_relu),
    BatchNormalization(),

    Dense(output_size, activation='linear'),
])

optimizer = tf.keras.optimizers.Adam(learning_rate=0.005, decay=5e-4)
model.compile(optimizer=optimizer,
              loss='mean_absolute_error',
              metrics=['MSE'])

hist = model.fit(X_train, Y_train,
          batch_size=32, epochs=200,
          validation_data=(X_val, Y_val))

model.evaluate(X_test, Y_test)[1]

print(model(X_test[0:10]).numpy(), Y_test[0:10])